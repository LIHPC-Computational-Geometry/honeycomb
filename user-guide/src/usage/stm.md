# Using maps in concurrent / parallel contexts

---

## Needs

Rust's ownership semantics require us to add synchronization mechanism to our structure if we want
to use it in concurrent contexts. Using primitives such as atomics and mutexes would be enough to
get programs to compile, but it would respectively yield an incorrect or impractical implementation:

- Atomics give guarantees on instructions interleaving for a single given variable, they do not give
  any guarantees for instructions affecting different atomic variables.
- Mutexes (and similar locks, e.g. RWLocks) can be used to create guarantees when accessing multiple
  variable: for example, we can write an operation that does not progress until all of the used data
  is locked. However, locks are error-prone, have very poor composability.

The nature of meshing operations disqualify both mechanisms. They are complex, access many
variables, and are often comprised of multiple steps. For example, the following operation is
executed on all affected attributes of a sew:

<figure style="text-align:center">
    <img src="../images/attribute_merge.svg" alt="Merge Operation" />
    <figcaption><i>Attribute merging operation. This occurs at each sew operation.</i></figcaption>
</figure>

Because the map can go through invalid intermediate states during a single operation, we need to
ensure another thread will not use one of these as the starting point for another operation. This
rules out atomic variables.

The sew operation is the main method used to create new connectivities in the map. This means that
most high-level meshing operations will call this method multiple times. If these meshing operations
require locking all of the variables to ensure correct execution, the locks must be returned or
exposed to the user so that he can unlock them at the right time. Manual lock management is
error-prone, and becomes impossible in practice for complex meshing operations.


## Software Transactional Memory

We choose to use Software Transactional Memory (STM) to handle high-level synchronization of
the structure. Unlike locks, STM has great composability and allows users of the crate to easily
define pseudo-atomic segments in their own algorithms.

Exposing an API that allows users to handle synchronization also means that the implementation
isn't bound to a given parallelization framework. Instead of relying on predefinite parallel
routines (e.g. a provided `parallel_for` on given cells), the structure can be used to implement
existing algorithms regardless of their approach (data-oriented, task-based, ...).


## Examples

To illustrate all of this, we provide two examples: one using `rayon`, and the other using
`std::thread`. The former focus exclusively on avoiding conflicts while the latter includes
transactions fallible due to meshing errors.


### Move all vertices to the average of their neighbors

In the following routine, we shift each vertex that's not on a boundary to the average of its
neighbors positions. In this case, transactions allow us to ensure we won't compute a new
position from a value that has been replaced since the start of the computation.


#### Code

```rust
{{#include snippets/parallel_shift.rs}}
```

#### Breakdown

The main map structure, `CMap2`, can be edited in parallel using transactions to ensure algorithm
correctness.

In the main computation loop, we use a transaction to ensure each new vertex value is computed from
the current neighbor's values. The errors generated by `read_vertex` and `write_vertex` are used to
(early) detect any changes to the data used in the transaction, here, the list of `neigh` vertices.

At the end of the transaction block, the commit routine will check again if any used data has been
altered. If not, results of the transaction will be validated and written to memory.

### Cut all squares of a grid into triangles

In the following routine, we generate an orthogonal grid and split all of its cells diagonally.
While no beta values should be edited concurrently, synchronization is necessary to protect the
integrity of *I*-cells and their bound attributes (here, spatial coordinates).

#### Code

```rust
{{#include snippets/parallel_cut.rs}}
```

#### Breakdown

In this example, we create batches of work for each thread to process. The exact reason we require
transactions here is due to STM implementation specificities. While some STM algorithms fully
prevent operating on invalid data (eager), others will not detect this until there is an attempt
to commit the transaction (lazy). The implementation we use is among the latter.

This implies that, if conflicting operations are executed concurrently, any check for invariants
we do in our algorithm can fail due to an inconsistent data state. Practically, we can simply
use a fallible transaction (that's `atomically_with_err`) to define our atomic segment, and handle
the error like any other.

In the above example, transactions are retried until success, since we can guarantee that only
valid data states are committed; that implies transactions will eventually succeed, albeit after
many retries.
